{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework, make sure that you format your notbook nicely and cite all sources in the appropriate sections. Programmatically generate or embed any figures or graphs that you need.\n",
    "\n",
    "Names: __Josh Alter, Robert Chatterton__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Word2Vec paper questions\n",
    "---------------------------\n",
    "### Describe how a CBOW word embedding is generated.\n",
    "- from section 3.1: A CBOW word embedding is generated by taking words within a ceratain window around the target word and creating a vector representing the context of that given target word.\n",
    "\n",
    "### What is a CBOW word embedding and how is it different from a skip-gram word embedding?\n",
    "- figure 1: The CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word. These embeddings are mirrors. The CBOW word embedding is meant to help generate the words surround that embedding (And thus is optimized to do that), whereas the skip-gram word embedding is made to generate the word given multiple context word embeddings. This means that those embeddings will be optimized for preidction of a missing word given this context.\n",
    "\n",
    "### What is the task that the authors use to evaluate the generated word embeddings?\n",
    "- section 4.1: we hate to reference such a large chunk from the paper, but its description of the task is incredibly digestable: \"Overall, there are 8869 semantic and 10675 syntactic questions. The questions in each category were created in two steps: first, a list of similar word pairs was created manually. Then, a large list of questions is formed by connecting two word pairs. For example, we made a list of 68 large American cities and the states they belong to, and formed about 2.5K questions by picking two word pairs at random. We have included in our test set only single token words, thus multi-word entities are not present (such as New York). We evaluate the overall accuracy for all question types, and for each question type separately (semantic, syntactic). Question is assumed to be correctly answered only if the closest word to the vector computed using the above method is exactly the same as the correct word in the question; synonyms are thus counted as mistakes.\" \n",
    "- They essentially just ask certain types of questions and carefully assess the answers comparing these results of their models to that of older models. Funny enough their new model alone did not perform better over, in fact they had to merge an older model with their new one to break the record. \n",
    "\n",
    "### What are PCA and t-SNE? Why are these important to the task of training and interpreting word embeddings?\n",
    "- A really good comparison of the two dimentionality reduction visualization techniques is done here 'https://www.geeksforgeeks.org/difference-between-pca-vs-t-sne/' \n",
    "- these are really important because they allow us to compare generated embeddings in a dimensionality space we can understand. This lets us contrsuct models that can be interpretted more easily. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Train your own word embeddings\n",
    "--------------------------------\n",
    "\n",
    "Spooky Authors Dataset\n",
    " - The spooky Authors Data set seems to be a compilation of text from horror stories by Edgar Allan Poe, Mary Shelley, and HP Lovecraft.\n",
    " \n",
    "Describe what data set you have chosen to compare and contrast with the Spooky Authors Dataset. Make sure to describe where it comes from and it's general properties.\n",
    " - We thought it best to compare this set of written text with that of an older, but just as well known set of stories - Shakespeare. We hope that there will more more symmetry between these two than that of text from some other medium (TV, reviews, etc...). We are also interested in analyzing how the morphology of some words has changed between Shakespeare's (1564-1616) era and that of someone like Edgar Allen Poe (1809-1849). Our Shakespeare dataset is his complete work of plays. Because each plays' genre certainly differs from the horror of the spooky set we look to see how a variation of genre compares to that of a single. Below is a link to where we found our Dataset\n",
    " - https://www.kaggle.com/kingburrito666/shakespeare-plays\n",
    " \n",
    "When training these word-embeddings, make reasonable decisions regarding text-normalization and pre-processing and describe what you have done.\n",
    " - we decided what we do to one we must do to the other, naturally. We tokenized every line of both datasets with nltk's word tokenize function. Then we case folded and removed all non alphabetic tokens. This was done to both as we look to assume that case of word shouldnt alter its meaning too heavily. We also assume that a non-alphabetic token wont be meaningful in any way for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T03:27:00.340250Z",
     "start_time": "2020-10-24T03:26:59.570883Z"
    }
   },
   "outputs": [],
   "source": [
    "# import your libraries here\n",
    "import gensim\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Train embedding on GIVEN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:25.438770Z",
     "start_time": "2020-10-24T04:39:24.888507Z"
    }
   },
   "outputs": [],
   "source": [
    "# code to train your word embeddings\n",
    "\n",
    "# Read the file 'spooky-author-identification/train.csv' and prepare the training data in the following format\n",
    "# imports\n",
    "import csv\n",
    "\n",
    "# code to train your word embeddings\n",
    "sentences = []\n",
    "\n",
    "with open(\"./train.csv\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for line in reader:\n",
    "        tokenized = nltk.word_tokenize(line[1]) #tokenize\n",
    "        no_punc = [word.lower() for word in tokenized if word.isalpha()] # remove non alphabetic chars and convert to all lowercase\n",
    "        \n",
    "        sentences.append(no_punc)\n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:38.482701Z",
     "start_time": "2020-10-24T04:39:28.044970Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# The dimension of word embedding. \n",
    "# This variable will be used throughout the program\n",
    "# you may vary this as you desire\n",
    "EMBEDDINGS_SIZE = 200\n",
    "\n",
    "# Training Word2Vec model from Gensim. \n",
    "# Below are the hyperparameters that are most relevant. But feel free to explore other \n",
    "# options too:\n",
    "# sg = 1\n",
    "# window = 5\n",
    "# size = EMBEDDING_SIZE\n",
    "# min_count = 1\n",
    "\n",
    "spooky_model = Word2Vec(sentences=sentences, window=5, size=EMBEDDINGS_SIZE, min_count=1, sg=1)\n",
    "spooky_model.save(\"word2vec_sooky.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:43.448249Z",
     "start_time": "2020-10-24T04:39:43.444835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 24978\n"
     ]
    }
   ],
   "source": [
    "# if you save your Word2Vec as the variable model, this will \n",
    "# print out the vocabulary size\n",
    "print('Vocab size {}'.format(len(spooky_model.wv.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n"
     ]
    }
   ],
   "source": [
    "for key in spooky_model.wv.vocab.keys():\n",
    "    print(key)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:48.730304Z",
     "start_time": "2020-10-24T04:39:45.451960Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can save file in txt format, then load later if you wish.\n",
    "spooky_model.wv.save_word2vec_format('embeddings.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Train embedding on YOUR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then do a second data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to C:\\Users\\Robert\n",
      "[nltk_data]     Chatterton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to train your word embeddings\n",
    "sentences_shakespeare = []\n",
    "\n",
    "with open(\"./Shakespeare_data.csv\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for line in reader:\n",
    "        tokenized = nltk.word_tokenize(line[5]) #tokenize\n",
    "        no_punc = [word.lower() for word in tokenized if word.isalpha()] # remove non alphabetic chars and convert to all lowercase\n",
    "        \n",
    "        sentences_shakespeare.append(no_punc)\n",
    "        \n",
    "f.close()\n",
    "\n",
    "# print(sentences[2:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dimension of word embedding. \n",
    "# This variable will be used throughout the program\n",
    "# you may vary this as you desire\n",
    "EMBEDDINGS_SIZE = 200\n",
    "\n",
    "# Training Word2Vec model from Gensim. \n",
    "# Below are the hyperparameters that are most relevant. But feel free to explore other \n",
    "# options too:\n",
    "# sg = 1\n",
    "# window = 5\n",
    "# size = EMBEDDING_SIZE\n",
    "# min_count = 1\n",
    "\n",
    "speare_model = Word2Vec(sentences=sentences_shakespeare, window=5, size=EMBEDDINGS_SIZE, min_count=1, sg=1)\n",
    "speare_model.save(\"word2vec_speare.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 22023\n"
     ]
    }
   ],
   "source": [
    "# if you save your Word2Vec as the variable model, this will \n",
    "# print out the vocabulary size\n",
    "print('Vocab size {}'.format(len(speare_model.wv.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playerline\n"
     ]
    }
   ],
   "source": [
    "# print out the first word with an associated embedding in the model trained on the given data\n",
    "\n",
    "for key in speare_model.wv.vocab.keys():\n",
    "    print(key)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can save file in txt format, then load later if you wish.\n",
    "speare_model.wv.save_word2vec_format('embeddings_mine.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Evaluate the differences between the word embeddings\n",
    "----------------------------\n",
    "Produce any graphs or figures that illustrate what you have found and write 2 - 3 paragraphs describing the differences and why you see them.\n",
    "#### Using these two graphs we can visualize the main differences between these two datasets, genre and time period. \n",
    "#### Paragraph 1 - difference 1 based on genre\n",
    "  - A good word to represetn this difference is 'cry'. We can do this only because we know beforehand the resepective genre breakdowns and We're not sure it would be obvious based on any certain word the genre of the text for other datasets. The closest neighbors of 'cry' in the spooky_model are recognised, heat, ligeia, shrieked, daylight and scream. The nearest neighbors of 'cry' in the speare_model are forsake, withdraw, determine, song, confound and tales. When comparing these two word lists we can see that shrieked and scream appear next to cry implying that people are crying in the same situations that they would be screaming and shrieking; this is something you would see in a horror stroy. Conversly we see cry associated with song and tales in shakespeare indicating that cry is used here as verb describing how verses are expressed. This is an interesting way to identify different genres of text. \n",
    "  \n",
    "#### Paragraph 2 - difference 2 based on time period\n",
    "  - A good word to represent this difference is 'to'. The closest neighbors of 'to' in the spooky_model are making, would, take, change, desire, and him. The closest neighbors of 'to' in the speare_model are thither, freely, unto, bids, yourselves, and directly. Intuitively you can posit this, but with TSNE we can easily visualize how the past few hundred years have forced changes to the English Language. The word 'to' is obviously here to stay, but who knows what other words will have similar meanings in 100 years. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6601321\n",
      "0.7352043\n"
     ]
    }
   ],
   "source": [
    "# test of cosine difference between two word embeddings \n",
    "# the degree distance each embedding if they were to be graphed in a high dimensional space\n",
    "print(spooky_model.wv.similarity('they', 'we'))\n",
    "print(speare_model.wv.similarity('they', 'we'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04464763  0.2317154  -0.15386432  0.10081241 -0.10362487  0.18057637\n",
      " -0.09024871 -0.48174432 -0.45373887  0.01797479  0.20947967  0.27366617\n",
      " -0.04766431 -0.0673878   0.32945946 -0.3215134   0.33892027  0.04526712\n",
      "  0.20038366  0.17237233 -0.33902717 -0.02810762 -0.2412441   0.25235513\n",
      " -0.05693242  0.44157818 -0.1102667  -0.26145124  0.03851573  0.17713201\n",
      " -0.00409068  0.17602332 -0.2121416  -0.41702142  0.06196535  0.22324207\n",
      " -0.16569753  0.05778773 -0.13316539 -0.17970489 -0.00182216  0.13275032\n",
      "  0.30225018 -0.2597866  -0.25130367  0.11290174  0.11804923  0.33036435\n",
      " -0.23246264  0.08995044  0.04524405 -0.04028279  0.04762954 -0.03776636\n",
      "  0.0681027   0.16787411 -0.02587365  0.17487611 -0.17335469  0.11689661\n",
      " -0.22668429  0.32606992 -0.06698465  0.25116476  0.42903167 -0.30857646\n",
      " -0.3429984   0.18879513  0.03508647  0.08223458 -0.34466213  0.25243407\n",
      "  0.16872719  0.2546734  -0.19397488  0.25973642 -0.2842471   0.05044419\n",
      " -0.23601805  0.13622817  0.03047802  0.0224433   0.04118426  0.00545684\n",
      " -0.08769985 -0.01153386  0.00464828  0.09962361 -0.23297839 -0.13764058\n",
      " -0.30576864 -0.305638    0.186454   -0.20929706  0.41826618  0.24761024\n",
      "  0.15704653 -0.0619029  -0.02415537 -0.260454    0.52636063 -0.03096284\n",
      " -0.35820735  0.14692195  0.01568963  0.08602802 -0.6643838   0.03984401\n",
      "  0.10404822 -0.29129463  0.2190033   0.10898917  0.16841613  0.36516094\n",
      " -0.2894648   0.02703792  0.20538454  0.09855963 -0.05405533  0.17932895\n",
      "  0.44714028 -0.2697122   0.3984131  -0.00088192  0.49994874 -0.12140738\n",
      " -0.35897523 -0.27694184 -0.07175432  0.18989877 -0.40968996 -0.26122853\n",
      " -0.08667333 -0.21653682  0.12731954 -0.13529505  0.23771483 -0.13069624\n",
      "  0.3402921   0.1809866  -0.20717344 -0.5309059   0.24355295 -0.2308135\n",
      "  0.08100633 -0.00069541 -0.25493884 -0.3138116  -0.0456772   0.35212082\n",
      " -0.26770696  0.13454184  0.07249966 -0.0431647   0.10284306 -0.2845339\n",
      " -0.00881039 -0.17199032  0.21904403 -0.10448907 -0.03317038 -0.23275428\n",
      "  0.59426105  0.13945341 -0.1397036   0.14352092 -0.15628393 -0.09768186\n",
      "  0.02482156 -0.19474295  0.04229346  0.5572929  -0.17758763 -0.1735619\n",
      "  0.27820185 -0.12833406 -0.21838504 -0.17105347  0.10654879 -0.05402747\n",
      " -0.17328058  0.24957676 -0.06920698  0.04931134 -0.08687706 -0.22849225\n",
      "  0.39753437 -0.03607371 -0.10922141 -0.26532394  0.03157318 -0.07143114\n",
      "  0.06073511  0.16728151  0.2905284   0.06205341  0.31303224 -0.21526264\n",
      " -0.33135816  0.01678055]\n",
      "[('elwood', 0.7907461524009705), ('gilman', 0.7761220335960388), ('himself', 0.7731051445007324), ('armitage', 0.7646495699882507), ('sleep', 0.7625279426574707), ('felix', 0.7623670101165771), ('boy', 0.7623326778411865), ('adrian', 0.7598520517349243), ('pickman', 0.7586260437965393), ('earl', 0.7539472579956055)]\n",
      "[-0.10172862 -0.0161985  -0.03621014  0.06020853  0.2593362   0.12231322\n",
      " -0.13545908 -0.22258769 -0.13486825 -0.05962203  0.2061438   0.24174333\n",
      " -0.00201885 -0.14703356  0.30477363 -0.01064945  0.02441906 -0.18740723\n",
      "  0.04537505 -0.2778173  -0.36803555 -0.2824489   0.12042728  0.15650234\n",
      "  0.1758453   0.12746371  0.12054572 -0.12818497 -0.00263501  0.18443754\n",
      " -0.07624545  0.13419656  0.07041595 -0.32117012  0.28985277 -0.04753103\n",
      " -0.3764778   0.00858391 -0.1405153  -0.44142872  0.07881503  0.07979759\n",
      "  0.18339771 -0.2667561   0.0753138   0.14806873  0.42692357  0.32282144\n",
      " -0.05914353 -0.03204969 -0.13543573 -0.22170633  0.22799441  0.19200325\n",
      " -0.39633963 -0.19746438  0.12274399  0.00209248 -0.24308932  0.01378074\n",
      " -0.3486851   0.12783453  0.19058947 -0.10553553  0.43407595 -0.24126643\n",
      " -0.30614823  0.08004601 -0.2113001   0.17492871 -0.33806866  0.20061038\n",
      " -0.02817171  0.06495538  0.06689713  0.05320036 -0.1216543   0.41731435\n",
      " -0.17810087  0.21599217 -0.1691502   0.31477788 -0.20250884 -0.07976234\n",
      " -0.19164585 -0.01913786  0.2479885   0.09618846 -0.06518286  0.05344708\n",
      " -0.37597203 -0.00600793  0.29314065 -0.1212442   0.12242592  0.31349236\n",
      " -0.11884215  0.38274273 -0.12593512 -0.17669934  0.37955248 -0.14721607\n",
      " -0.12147138  0.08560879 -0.23026195  0.22181375 -0.6528918  -0.13192528\n",
      " -0.20303512  0.09264641  0.18080851 -0.04452761 -0.25811085  0.26287645\n",
      " -0.307796    0.19788396 -0.0812811  -0.25922585 -0.23207699 -0.12833455\n",
      " -0.02762956 -0.4808236   0.41802418 -0.3004371   0.02847477  0.00355495\n",
      " -0.3900653  -0.02193176 -0.29171968  0.16952759  0.2473729  -0.04751743\n",
      " -0.07670028 -0.19043298 -0.26549318 -0.5627721   0.2540977   0.03676282\n",
      "  0.13039663  0.06069001 -0.18690848 -0.4459649   0.38587636 -0.14708242\n",
      "  0.23191746  0.11004829 -0.16583595 -0.16317458  0.12955125  0.27771333\n",
      " -0.02511495  0.41999334  0.2691323   0.09769961 -0.09810393 -0.23606433\n",
      " -0.23747623 -0.4523217   0.18148035 -0.08337744 -0.04295311  0.01722148\n",
      "  0.5499577   0.176212    0.08469096  0.18149191  0.05415718 -0.31772998\n",
      "  0.1347786  -0.45059118 -0.4623511   0.5524254  -0.34212717 -0.3430421\n",
      " -0.0504444   0.02796125 -0.2109242  -0.02925083 -0.0502236  -0.16023508\n",
      " -0.31300008  0.4916306  -0.07727581  0.33956534 -0.05561215  0.19324575\n",
      "  0.4742881   0.30596274  0.2700953   0.12945653 -0.25431976 -0.0475659\n",
      "  0.26598915  0.12857042 -0.2737809  -0.0591826   0.41330037 -0.11675596\n",
      " -0.65294755  0.23081285]\n",
      "[('himself', 0.8025932908058167), ('she', 0.7611709833145142), ('it', 0.7542572021484375), ('who', 0.7423742413520813), ('hector', 0.73726487159729), ('otherwise', 0.7254471778869629), ('something', 0.7253339290618896), ('jove', 0.7233778238296509), ('madness', 0.7182927131652832), ('wisely', 0.7166516184806824)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-f4e33e052854>:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  print(spooky_model['he']) # word embedding of 'he' from first trained model\n",
      "<ipython-input-16-f4e33e052854>:5: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  print(spooky_model.similar_by_word('he')) # all similar words (n-similar) to 'he'\n",
      "<ipython-input-16-f4e33e052854>:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  print(speare_model['he']) # word embedding of 'he' from second training model\n",
      "<ipython-input-16-f4e33e052854>:7: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  print(speare_model.similar_by_word('he')) # all similar words (n-similar) to 'he'\n"
     ]
    }
   ],
   "source": [
    "# check to see if we need to normalize more. Ideally the similar words should not be the same word in another \n",
    "# form (i.e. 'he' and 'He')\n",
    "\n",
    "print(spooky_model['he']) # word embedding of 'he' from first trained model \n",
    "print(spooky_model.similar_by_word('he')) # all similar words (n-similar) to 'he'\n",
    "print(speare_model['he']) # word embedding of 'he' from second training model\n",
    "print(speare_model.similar_by_word('he')) # all similar words (n-similar) to 'he'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting TSNE with a given model and word\n",
    "# reducing this high dimensionality data representation to a 2d graph\n",
    "def display_closestwords_tsnescatterplot(model, word):\n",
    "    \n",
    "    arr = np.empty((0,200), dtype='f')\n",
    "    word_labels = [word]\n",
    "\n",
    "    # get close words\n",
    "    close_words = model.similar_by_word(word)\n",
    "    \n",
    "    # add the vector for each of the closest words to the array\n",
    "    arr = np.append(arr, np.array([model[word]]), axis=0)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    # find tsne coords for 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    # display scatter plot\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-22c3db3fcd59>:9: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  close_words = model.similar_by_word(word)\n",
      "<ipython-input-17-22c3db3fcd59>:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  arr = np.append(arr, np.array([model[word]]), axis=0)\n",
      "<ipython-input-17-22c3db3fcd59>:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  wrd_vector = model[wrd_score[0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot with first model of some word ('he')\n",
    "plt.cla()\n",
    "display_closestwords_tsnescatterplot(spooky_model, 'he')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-22c3db3fcd59>:9: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  close_words = model.similar_by_word(word)\n",
      "<ipython-input-17-22c3db3fcd59>:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  arr = np.append(arr, np.array([model[word]]), axis=0)\n",
      "<ipython-input-17-22c3db3fcd59>:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  wrd_vector = model[wrd_score[0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot with first model of some word ('he') - **FIXME** overwrites first plot \n",
    "plt.cla()\n",
    "display_closestwords_tsnescatterplot(speare_model, 'he')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cite your sources: https://medium.com/@aneesha/using-tsne-to-plot-a-subset-of-similar-words-from-word2vec-bb8eeaea6229\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Feedforward Neural Language Model\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) First, encode  your text into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T21:39:09.625031Z",
     "start_time": "2020-10-26T21:39:09.009109Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing utility functions from Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Input\n",
    "\n",
    "NGRAM = 3 # The size of the ngram language model you want to train\n",
    "\n",
    "# Initializing a Tokenizer\n",
    "# It is used to vectorize a text corpus. Here, it just creates a mapping from \n",
    "# word to a unique index. (Note: Indexing starts from 0)\n",
    "# Example:\n",
    "data_1 = sentences\n",
    "tokenizer_1 = Tokenizer()\n",
    "tokenizer_1.fit_on_texts(data_1)\n",
    "encoded_1 = tokenizer_1.texts_to_sequences(data_1)\n",
    "\n",
    "data_2 = sentences_shakespeare\n",
    "tokenizer_2 = Tokenizer()\n",
    "tokenizer_2.fit_on_texts(data_2)\n",
    "encoded_2 = tokenizer_2.texts_to_sequences(data_2)\n",
    "\n",
    "# Make sure to include the padding token in your vocabulary size - NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Next, prepare your sequences from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed ngram based sequences (Used for Feedforward)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The training samples will be structured in the following format. \n",
    "Depening on which ngram model we choose, there will be (n-1) tokens \n",
    "in the input sequence (X) and we will need to predict the nth token (Y)\n",
    "\n",
    "            X,\t\t\t\t\t\t  y\n",
    "    this,    process               however\n",
    "    process, however               afforded\n",
    "    however, afforded\t           me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngram_training_samples(model, encoded_data, n=3): #where n is the context words or n-gram - 1\n",
    "    '''\n",
    "    Takes the encoded data (list of lists) and generates the training samples\n",
    "    out of it.\n",
    "    Parameters:\n",
    "    up to you!\n",
    "    return: list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n",
    "    '''\n",
    "    samples = list()\n",
    "    for sentence in encoded_data:\n",
    "        if len(sentence) < n or sentence == [] or sentence == None:\n",
    "            continue\n",
    "        #print(sentence)\n",
    "        for i in range(n, len(sentence)):\n",
    "            tmp_sample_words = [model.wv.index2word[idx] for idx in sentence[i-n:i]] # gets n-1 gram in encoded and converts\n",
    "            samples.append(tmp_sample_words)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19580\n",
      "['on', 'sixteen', 'thing']\n"
     ]
    }
   ],
   "source": [
    "print(len(encoded_1))\n",
    "x = generate_ngram_training_samples(spooky_model, encoded_1[:-3])\n",
    "for l in x:\n",
    "    print(l)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Then, split the sequences into X and y and create a Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:31.213422Z",
     "start_time": "2020-10-24T05:21:31.061759Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note here that the sequences were in the form: \n",
    "# sequence = [x1, x2, ... , x(n-1), y]\n",
    "# We still need to separate it into [[x1, x2, ... , x(n-1)], ...], [y1, y2, ...]\n",
    "def split_samples(ngram_samples, n=NGRAM):\n",
    "    '''\n",
    "    Splits a list of lists of ngrams into a list of lists of n-1 grams, followed by the corresponding array of y-values\n",
    "    '''\n",
    "    splits = []\n",
    "    y = []\n",
    "    for gram in ngram_samples:\n",
    "        x = gram[:n - 1]\n",
    "        splits.append(x)\n",
    "        y.append(gram[n - 1])\n",
    "    splits.append(y)\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:34.675827Z",
     "start_time": "2020-10-24T05:21:33.315288Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_embeddings(model):\n",
    "    '''Loads and parses embeddings trained in earlier.'''\n",
    "    \n",
    "    # you may find generating the following two dicts useful:\n",
    "    # word to embedding : {'the':1, ...}\n",
    "    # index to embedding : {1:'the', ...} (inverse of word_2_embedding)\n",
    "    word_to_embedding = {}\n",
    "    index_to_embedding = {}\n",
    "    for i in range(len(model.wv.vocab)):\n",
    "        word = model.wv.index2word[i]\n",
    "        \n",
    "        word_to_embedding[word] = (i, model[word])\n",
    "        index_to_embedding[i] = (word, model[word])\n",
    "\n",
    "    return word_to_embedding, index_to_embedding\n",
    "\n",
    "# remember that \"0\" index is assigned for padding token. \n",
    "# Hence, initialize the vector for padding token as all zeros of embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:22:24.016237Z",
     "start_time": "2020-10-24T05:22:24.011220Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_generator(X, y, num_sequences_per_batch, steps_per_epoch, model):\n",
    "    '''\n",
    "    Returns data generator to be used by feed_forward\n",
    "    https://wiki.python.org/moin/Generators\n",
    "    https://realpython.com/introduction-to-python-generators/\n",
    "    \n",
    "    Yields batches of embeddings and labels to go with them.\n",
    "    Use one hot vectors to encode the labels (see the to_categorical function)\n",
    "    Thanks for the help in OH!\n",
    "    '''\n",
    "    we, ie = read_embeddings(model)\n",
    "    final_X = []\n",
    "    final_y = []\n",
    "    while(True):\n",
    "        for i in range(len(X)):\n",
    "            if len(final_X) == num_sequences_per_batch:\n",
    "                yield np.array(final_X), np.array(final_y)\n",
    "                final_X, final_y = [], []\n",
    "            \n",
    "            eL = [we[word][1] for word in X[i]]\n",
    "            cEL = np.concatenate(eL)\n",
    "            final_X.append(cEL)\n",
    "            yOneHot = to_categorical(we[y[i]][0], num_classes=len(ie))\n",
    "            final_y.append(yOneHot)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:22:55.470133Z",
     "start_time": "2020-10-24T05:22:55.398259Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-3b154f87a5e3>:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  word_to_embedding[word] = (i, model[word])\n",
      "<ipython-input-24-3b154f87a5e3>:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  index_to_embedding[i] = (word, model[word])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (1024, 400)\n",
      "y shape:  (1024, 24978)\n"
     ]
    }
   ],
   "source": [
    "num_sequences_per_batch = 1024 # or Batch Size\n",
    "\n",
    "# initialize data_generator for spooky model\n",
    "spooky_vsize = len(spooky_model.wv.vocab)\n",
    "spooky_sequences = generate_ngram_training_samples(spooky_model, encoded_1[:-3])\n",
    "X_spooky = split_samples(spooky_sequences)[:-1]\n",
    "y_spooky = split_samples(spooky_sequences)[-1]\n",
    "steps_per_epoch_spooky = len(spooky_sequences) // num_sequences_per_batch  # Number of batches per epoch\n",
    "train_generator_spooky = data_generator(X_spooky, y_spooky, num_sequences_per_batch, steps_per_epoch_spooky, spooky_model)\n",
    "\n",
    "# initialize data_generator for Shakespeare model\n",
    "speare_vsize = len(speare_model.wv.vocab)\n",
    "speare_sequences = generate_ngram_training_samples(speare_model, encoded_2[:-3])\n",
    "X_speare = split_samples(speare_sequences)[:-1]\n",
    "y_speare = split_samples(speare_sequences)[-1]\n",
    "steps_per_epoch_speare = len(speare_sequences) // num_sequences_per_batch  # Number of batches per epoch\n",
    "train_generator_speare = data_generator(X_speare, y_speare, num_sequences_per_batch, steps_per_epoch_speare, speare_model)\n",
    "\n",
    "sample=next(train_generator_spooky) # this is how you get data out of generators\n",
    "print(\"X shape: \", sample[0].shape) # (batch_size, (n-1)*EMBEDDING_SIZE)\n",
    "print(\"y shape: \", sample[1].shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Train your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:56:19.207252Z",
     "start_time": "2020-10-24T04:56:19.204894Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T03:56:50.919869Z",
     "start_time": "2020-10-24T03:56:50.779792Z"
    }
   },
   "outputs": [],
   "source": [
    "# code to train a feedforward neural language model \n",
    "# on a set of given word embeddings\n",
    "# make sure not to just copy + paste to train your two \n",
    "def create_model(embedding_model):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dense(len(embedding_model.wv.vocab), activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Defining the model architecture using Keras Sequential API\n",
    "modelSpooky = create_model(spooky_model)\n",
    "modelSpeare = create_model(speare_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:01:50.757170Z",
     "start_time": "2020-10-24T03:56:53.620836Z"
    }
   },
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:01:50.757170Z",
     "start_time": "2020-10-24T03:56:53.620836Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-29-ac28bb1b628d>:2: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "452/452 [==============================] - 163s 361ms/step - loss: 7.0672 - accuracy: 0.0824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f979aded30>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train.csv model\n",
    "modelSpooky.fit_generator(train_generator_spooky, \n",
    "                    steps_per_epoch=steps_per_epoch_spooky,\n",
    "                    epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:01:50.757170Z",
     "start_time": "2020-10-24T03:56:53.620836Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-3b154f87a5e3>:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  word_to_embedding[word] = (i, model[word])\n",
      "<ipython-input-24-3b154f87a5e3>:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  index_to_embedding[i] = (word, model[word])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470/470 [==============================] - 143s 304ms/step - loss: 6.7289 - accuracy: 0.0459\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f9271a7a30>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shakespeare_data.csv model\n",
    "modelSpeare.fit_generator(train_generator_speare, \n",
    "                    steps_per_epoch=steps_per_epoch_speare,\n",
    "                    epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                25664     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24978)             1623570   \n",
      "=================================================================\n",
      "Total params: 1,649,234\n",
      "Trainable params: 1,649,234\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 64)                25664     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 22023)             1431495   \n",
      "=================================================================\n",
      "Total params: 1,457,159\n",
      "Trainable params: 1,457,159\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelSpooky.summary()\n",
    "modelSpeare.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Generate Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:13:54.425934Z",
     "start_time": "2020-10-24T04:13:54.418616Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate a sequence from the model\n",
    "def generate_seq(model, tokenizer, seed, n_words, we):\n",
    "    '''\n",
    "    Parameters:\n",
    "        model: your neural network\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        seed: [w1, w2, w(n-1)]\n",
    "        n_words: generate a sentence of length n_words\n",
    "    Returns: string sentence\n",
    "    '''\n",
    "    vocab = [k for k, v in we.items()]\n",
    "    sentence = seed\n",
    "    vect = calc_vec(seed, we)\n",
    "    prev = sentence\n",
    "    for w in range(n_words - len(seed)):\n",
    "        probs = model.predict(vect)[0]\n",
    "        prediction = np.random.choice(vocab, p=probs)\n",
    "        sentence.append(prediction)\n",
    "        prev = sentence[-(NGRAM - 1):]\n",
    "        vect = calc_vec(prev, we)\n",
    "    return ' '.join(sentence) + '.'\n",
    "\n",
    "def calc_vec(words, we):\n",
    "    vecs = []\n",
    "    for w in words:\n",
    "        vecs.append(we[w][1])\n",
    "    vect = np.concatenate(vecs)\n",
    "    vect = np.reshape(vect, (1,400))\n",
    "    return vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:13:54.425934Z",
     "start_time": "2020-10-24T04:13:54.418616Z"
    }
   },
   "source": [
    "### Generate Sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-3b154f87a5e3>:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  word_to_embedding[word] = (i, model[word])\n",
      "<ipython-input-24-3b154f87a5e3>:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  index_to_embedding[i] = (word, model[word])\n"
     ]
    }
   ],
   "source": [
    "weSpook, ieSpook = read_embeddings(spooky_model)\n",
    "weSpeare, ieSpeare = read_embeddings(speare_model)\n",
    "sents_spooky = []\n",
    "sents_speare = []\n",
    "for i in range(50):\n",
    "    sents_spooky.append(generate_seq(modelSpooky, tokenizer_1, [\"we\", \"are\"], 20, weSpook))\n",
    "    sents_speare.append(generate_seq(modelSpeare, tokenizer_2, [\"we\", \"are\"], 20, weSpeare))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) Compare your generated sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "spooky sentences:\n",
      "1:  we are change and it portion tottering i value my in wars peasants return had my of duke i pervaded.\n",
      "2:  we are dank and new our long and of quasi hands to heat fallen and of canopy to foster is.\n",
      "3:  we are centre to continue averted and preparations them he and at this own my with strove to which solid.\n",
      "4:  we are number some for an deter separate as had tale turned born if to rest that with mutter management.\n",
      "5:  we are over me reflection i he curious me some silence once submit my then life home i would its.\n",
      "6:  we are harp and italian an sworn me of these aside for by very of receiving his was undeceive for.\n",
      "7:  we are enchanted from as stronger beneath walking at to escaping gloom with among contradictory and companions had alive which.\n",
      "8:  we are moor i are pieces was speak principles is of suddenly to diabolic planned home full was see my.\n",
      "9:  we are steeds suffered is of reflection and of surmounting that it explicit his thought much seemed were terrific me.\n",
      "10:  we are strong at of fortunately received and of sea and of perceive and my nightmare she which shining i.\n",
      "11:  we are in daily by that patient longing seemed yet nassau town existence quarter it fact of as less she.\n",
      "12:  we are i as arranged congeries me been on opinion heart have and acknowledgement become to fells toiled larger white.\n",
      "13:  we are think and employment hospital at conception or length and of voyages end made headlong i unnamed to indescribable.\n",
      "14:  we are confusion be that walking i left desire glen affection and of keen and gardens was various single kinsfolk.\n",
      "15:  we are increase spots i dark my d his all of fever he things myself wakeful merely subalterns his winter.\n",
      "16:  we are seeming and lips to curious into celephaïs these left foredoomed that midsummer a only her reference his me.\n",
      "17:  we are were seldom his an figure hectic notice of receded silence learning works to you served away almost of.\n",
      "18:  we are candidates you i can were repulse come had duties an remainder and castro happened with beloved strange when.\n",
      "19:  we are judges and all imposture was stories altogether this replied furiously of was clothed to previous length land is.\n",
      "20:  we are wide secret he he ish a budding defences drums jermyn its pass me sister of pro appeal he.\n",
      "21:  we are resident seemed all bugs davy far play intolerable with hollow than is acquaintance be which comply no had.\n",
      "22:  we are without rolled some alluded seemed upon on tawdry hour first terrible had whiff set id of foetid and.\n",
      "23:  we are it conciliating cabin me from of ground scrutinized head i well in sickly to planets and it was.\n",
      "24:  we are mother one i in has queer to every not and bit discomfort me some brown he to trouble.\n",
      "25:  we are were when noxious was years me on you while for of attack and upon aside utter strange but.\n",
      "26:  we are threats for on essentially distant great curvature most of histories fatal who of within sign i increasing quick.\n",
      "27:  we are flooded only passed still of hurricane my horrors upon satan for of little might what delight existence but.\n",
      "28:  we are their to of been presence me great but room thenceforward that occurred then some let would scientific still.\n",
      "29:  we are them to donna tangible patience of of were style to metamorphoses concealed wife not when energy stretched was.\n",
      "30:  we are feet sake as dingy was of spirituality exhalation footsteps i burned there clara my worthy i six you.\n",
      "31:  we are was of ocean clerval third to fever he clew above his of several it us such practices there.\n",
      "32:  we are upon causes children her burned was through sinister not clothes i my of books and exercise even and.\n",
      "33:  we are terror and lay to england weights i did idris as corresponding only with lean of form diabolism was.\n",
      "34:  we are must peters to wood to in spit gigantic he had that trees mound of passed attentively at step.\n",
      "35:  we are yard how for years were rushing was suspected am goosetherumfoodle my adjacent my appeared any alluded to a.\n",
      "36:  we are of rise and as ruffians thing heard elopement holberg dream received penning no to had rocks long that.\n",
      "37:  we are had gravel be to house patient and be to seemed but made for rapid them yes my account.\n",
      "38:  we are as my or glimpse face for agony freely ancient you of legitimate nature doublet villain harbourage and blackwood.\n",
      "39:  we are concealment and it her there impressed moments spoke mad were spiritual happiest at political and in two not.\n",
      "40:  we are mortals undulation yelping siege closed identify few been door is visitors only his peasant from we slept yet.\n",
      "41:  we are scarcely after that companionship that in nerve i pocket pictures was of prepared reduced dunwich lovely such always.\n",
      "42:  we are by arrived are children this know and of like and found and words of recorded and of between.\n",
      "43:  we are until cobbler i outside cameleopard by upon xii distance called us ought sheared withstand half had mon we.\n",
      "44:  we are true in long she accustomed dwelling in atmosphere even your of subtractions river suspicion seven white by about.\n",
      "45:  we are words of side but of for have despondency why dangerous lucid more one there truly haunting vague i.\n",
      "46:  we are earth supposed at attempts is with lay permitted for kate participate determined his of continued scenery it came.\n",
      "47:  we are planned of wallet to means to of feeling any labour was of of itself and of stung to.\n",
      "48:  we are while illustrious his my motion importunities sailed has him conflict of their features for conjure is in doubtless.\n",
      "49:  we are hanging ultimate sensitive i would night was of creating restoring no not found to to in hovel i.\n",
      "50:  we are measure from on human best and of another mutual to still my expected that then i rushing at.\n",
      "\n",
      "shakespeare sentences\n",
      "1:  we are excepted swiftest bocchus sorted ass first caesar be a but me is and tawny they lady so is.\n",
      "2:  we are to you an ape master friends killed that yet forgive to six or as me him of grace.\n",
      "3:  we are by only eyes my and none me brace this natures of this sullens thou seats to motion enter.\n",
      "4:  we are so she c me before my swords they daughter good ballad help i signify to as wrestler to.\n",
      "5:  we are in have but being to never us for for die it end with bare direct of desert or.\n",
      "6:  we are with of ay bring of bolingbroke is and maiden a arthur for meet answer your become you soldiers.\n",
      "7:  we are in or father not to be villain cruel three to statue milch o thee advocate my endure by.\n",
      "8:  we are come stony i and parrot with abide noble being none to not he strong my you you bellow.\n",
      "9:  we are with run we concluded my your convocation persons enow unto challenges give your from of root well happen.\n",
      "10:  we are of man hath down if you talbot addrest mounseur brutish execute that you gentle it his every where.\n",
      "11:  we are too and a i that come mine for beware heaven not it mend is forms it no and.\n",
      "12:  we are a will tongue page here house i natural with it and here word hand how we fan a.\n",
      "13:  we are this demetrius cheapest and footing shuffling why no hand a and do pawn fifth thy as letters came.\n",
      "14:  we are it slightly vulture it does her grandsires force sighing i his of with will will our thou we.\n",
      "15:  we are king thee one befits rome gone him must executed crown should have ears of with mine i to.\n",
      "16:  we are my he i faculties without these fierce a robb christophero me relieved fortune praise and peep go feels.\n",
      "17:  we are a that made be and lucentio will enemies him was messenger worst all propre excellent tire uncle o.\n",
      "18:  we are now it from every beseech more and gloucester bathe in gods true my if two with if to.\n",
      "19:  we are execution dare your why for a that distraught be lips him and thou a brother to my salt.\n",
      "20:  we are he devils my art of grace peace me such play can world my imbecility so ruffian brought in.\n",
      "21:  we are which me may make your to as you for venture them of were but no be of spirits.\n",
      "22:  we are grief down of put in emperor play this way witch oxford in your obedience these i much was.\n",
      "23:  we are money not again old them not be enter to colour thou you ripen king am matron he mark.\n",
      "24:  we are night forsooth thy down great but with were some she forth here to whiffler thee repulsed turn of.\n",
      "25:  we are will it am my to and or set eat father to wiltshire i with me his will my.\n",
      "26:  we are not and joint come cymbeline and daughter to as since thy gentlemen shalt will of to thy reflection.\n",
      "27:  we are it you to my it violets me of at thy hence he protest away be three which thy.\n",
      "28:  we are him dead then let on crown finds i haunt dear my none o commendable not with or demean.\n",
      "29:  we are an have to then time i you his bring richard with alas did o worthily i nor my.\n",
      "30:  we are me troth dignity give sights now upon lacedaemon nominativo stories confine bring no and members i scutcheon tavy.\n",
      "31:  we are your horns it but bloods him that amongst i alas than election be be what scene shalt of.\n",
      "32:  we are that grave that must captain weep that her my end how of day present and kneels i your.\n",
      "33:  we are and gloucester in shall be have my have do knows have and led grace it most basket obeying.\n",
      "34:  we are without that it and piercing divided and chain a precious are methinks thus with that drunk thou robin.\n",
      "35:  we are to break he thousand a with i doubt truth see thou my hear suit is halters a he.\n",
      "36:  we are with give well he honour paris i foes i is which for forsworn not and stern in i.\n",
      "37:  we are i present she care now for reasons they once good whom she i duke offendendo in suspect accuse.\n",
      "38:  we are i briton mutiny by thy an left them your perceive of this pearl duteous thou wish when and.\n",
      "39:  we are hath where to here our fill all myself bored for lamp never is his on unthankfulness earth meaning.\n",
      "40:  we are with thine suitors no all is at make thou and king a i wade manner sovereignty assaulted good.\n",
      "41:  we are friend what service most affection sure you citizens enter with god true to he scene a of chin.\n",
      "42:  we are of at her of him i that blow but o at than my good lady conscience a never.\n",
      "43:  we are out weapon i very give sent it or debate with not of too to as place might my.\n",
      "44:  we are i boy friend o take fair to an unpeople thy tied he beggars do him my we die.\n",
      "45:  we are these or of my speak who i heaven head true my thee truly will escaped his did slew.\n",
      "46:  we are it i waxen age exeunt him a truce down worthy of here that be but carriages we you.\n",
      "47:  we are i thy in have this last fellow a your by ever these no me we but so purpose.\n",
      "48:  we are disposition of he of disease thee derby be you i crow lines is and censured lord uplifted as.\n",
      "49:  we are a with they another earth yet not house your excellently me as be we grief where exchange chamber.\n",
      "50:  we are or i it friends not but dare with may a forward be do me you fleance play which.\n",
      "\n",
      "finished generating.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nspooky sentences:\")\n",
    "[print(str(i + 1) + \": \", s) for i, s in enumerate(sents_spooky)]\n",
    "\n",
    "print(\"\\nshakespeare sentences\")\n",
    "[print(str(i + 1) + \": \", s) for i, s in enumerate(sents_speare)]\n",
    "\n",
    "print(\"\\nfinished generating.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally, once you have your trained language models, you will use both to generate sentences. Compare these with sentences that could be produced using Shannon's method with the statistical n-gram language models that we implemented earlier in the semester. Do your neural language models produce n-grams that were not observed during training? (1 paragraph, you may support this answer with code as desired). \n",
    " - These sentences are almost just bundles of words with much less meaning than those generated by Shannon's method. Our neural language models produce n-grams that are almost exclusively not seen during training. This is due to its large innacurracy. We can also hypothisize that this innacuracy is not only related to how feed forward models inherintly work, but are exacerbated by the sheer amount of data we train on. When we are comparing to Shannon's method we trained on much smaller sets of data and thus weighted the trained n-gram much higher when choosing a word. That word held a larger area of the picking zone persay and because of this we would see n-gram groupings very often. This gave the sentences good local word connections, but poor global sentence meaning. Here, with our low-accuracy feed-forward model we see neither good local word connection nor good global sentence meaning, rather just a group of words that could be produced given a vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources Cited\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
